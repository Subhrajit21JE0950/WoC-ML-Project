ML Project IIT(ISM) DHANBAD
Report  :                                                                                                                                 
This is the report about my findings regarding the project:
â€¢	Linear Regression uses gradient descent method to continuously update parameters. The hyperparameters are learning rate and number of iterations. I have found that
increasing learning rate makes the cost function minimise rapidly but after a certain point it oscillates. Whereas lower learning rate makes the cost function minimise
very slowly. The correct combination is found by hit and trial method. I have also used standardization on the training data. The cost function is 2522.
<img src="https://camo.githubusercontent.com/..." data-canonical-src="![image](https://user-images.githubusercontent.com/103888763/163830255-8e037e68-fc59-46b3-9f30-9cb4278a08c5.png)" width="200" height="400" />



